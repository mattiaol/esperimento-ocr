
from __future__ import annotations

import json
import os
import re
import tempfile
import unicodedata
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from pathlib import Path

import cv2
import numpy as np
import streamlit as st
from PIL import Image, ImageOps


# Reduce noisy TensorFlow logs (only affects keras_ocr/tensorflow imports)
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")

import pillow_heif

# Enable HEIC/HEIF support in Pillow
pillow_heif.register_heif_opener()


# ----------------------------
# Streamlit setup
# ----------------------------
st.set_page_config(page_title="TC ID Card OCR", page_icon="üÜî", layout="wide")
st.title("TC ID Card OCR Scanner")
st.markdown("Carica un'immagine (JPG/PNG/HEIC) e avvia l'estrazione dei dati.")


# ----------------------------
# Optional repo imports (graceful)
# ----------------------------
_FACE_FACTORY_AVAILABLE = False
_UTILS_AVAILABLE = False

try:
    # detect_face.py exposes: face_factory(face_model="ssd") -> FaceFactory
    from detect_face import face_factory  # type: ignore
    _FACE_FACTORY_AVAILABLE = True
except Exception:
    _FACE_FACTORY_AVAILABLE = False

try:
    # utlis.py (repo) contains perspective + rotation helpers used by the pipeline
    import utlis  # type: ignore
    _UTILS_AVAILABLE = True
except Exception:
    _UTILS_AVAILABLE = False


# ----------------------------
# Helpers
# ----------------------------
def load_image(uploaded_file) -> Optional[Image.Image]:
    """Load any uploaded image as a PIL.Image (handles HEIC thanks to pillow-heif)."""
    try:
        image = Image.open(uploaded_file)
        image = ImageOps.exif_transpose(image)
        if image.mode != "RGB":
            image = image.convert("RGB")
        return image
    except Exception as e:
        st.error(f"Errore nella lettura del file: {e}")
        return None


def pil_to_bgr(pil_img: Image.Image) -> np.ndarray:
    """Convert PIL RGB image to OpenCV BGR."""
    rgb = np.array(pil_img)
    return cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)


def normalize_text(s: str) -> str:
    """Uppercase + remove diacritics + compact spaces."""
    if s is None:
        return ""
    s = str(s).strip().replace("\n", " ").replace("\t", " ")
    s = re.sub(r"\s+", " ", s).strip().upper()
    s = unicodedata.normalize("NFKD", s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    return s


def safe_regex_find(pattern: str, text: str) -> Optional[str]:
    m = re.search(pattern, text)
    return m.group(0) if m else None


@dataclass
class OcrItem:
    text: str
    text_norm: str
    conf: float
    x1: int
    y1: int
    x2: int
    y2: int

    @property
    def cx(self) -> float:
        return (self.x1 + self.x2) / 2.0

    @property
    def cy(self) -> float:
        return (self.y1 + self.y2) / 2.0


# ----------------------------
# OCR backends (SIMPLE mode)
# ----------------------------
@st.cache_resource(show_spinner=False)
def _get_easyocr_reader():
    import easyocr  # local import
    return easyocr.Reader(["tr", "en"], gpu=False)


def run_easyocr(img_rgb: np.ndarray) -> List[OcrItem]:
    reader = _get_easyocr_reader()
    results = reader.readtext(img_rgb)  # [(bbox4pts, text, conf), ...]
    items: List[OcrItem] = []
    for bbox, text, conf in results:
        pts = np.array(bbox).astype(int)
        x1 = int(pts[:, 0].min())
        y1 = int(pts[:, 1].min())
        x2 = int(pts[:, 0].max())
        y2 = int(pts[:, 1].max())
        items.append(
            OcrItem(
                text=str(text),
                text_norm=normalize_text(str(text)),
                conf=float(conf),
                x1=x1,
                y1=y1,
                x2=x2,
                y2=y2,
            )
        )
    return items


def run_tesseract(img_rgb: np.ndarray) -> List[OcrItem]:
    import pytesseract  # local import

    # Try Turkish + English; fall back to English if needed
    langs = ["tur+eng", "tur", "eng"]
    data = None
    last_err = None
    for lang in langs:
        try:
            data = pytesseract.image_to_data(img_rgb, output_type=pytesseract.Output.DICT, lang=lang)
            last_err = None
            break
        except Exception as e:
            last_err = e
            data = None

    if data is None:
        raise RuntimeError(f"Tesseract non disponibile o errore di esecuzione: {last_err}")

    items: List[OcrItem] = []
    n = len(data.get("text", []))
    for i in range(n):
        txt = data["text"][i]
        if not txt or not str(txt).strip():
            continue
        conf_raw = data.get("conf", [0] * n)[i]
        try:
            conf = float(conf_raw)
        except Exception:
            conf = 0.0
        x1 = int(data["left"][i])
        y1 = int(data["top"][i])
        w = int(data["width"][i])
        h = int(data["height"][i])
        items.append(
            OcrItem(
                text=str(txt),
                text_norm=normalize_text(str(txt)),
                conf=conf,
                x1=x1,
                y1=y1,
                x2=x1 + w,
                y2=y1 + h,
            )
        )
    return items


def find_label(items: List[OcrItem], label_variants: List[str]) -> Optional[OcrItem]:
    variants = [normalize_text(v) for v in label_variants]
    for it in items:
        if it.text_norm in variants:
            return it
    for it in items:
        for v in variants:
            if v and v in it.text_norm and len(it.text_norm) <= len(v) + 3:
                return it
    return None


def pick_line_words(items: List[OcrItem], y_center: float, y_tol: float, min_x: float) -> List[OcrItem]:
    same_line = [it for it in items if abs(it.cy - y_center) <= y_tol and it.x1 >= int(min_x)]
    same_line.sort(key=lambda it: it.x1)
    return same_line


def join_words(words: List[OcrItem]) -> str:
    if not words:
        return ""
    s = " ".join(w.text_norm for w in words)
    return re.sub(r"\s+", " ", s).strip()


def extract_fields_simple(items: List[OcrItem], img_shape_hw: Tuple[int, int]) -> Tuple[Dict[str, str], Dict[str, Tuple[int, int, int, int]]]:
    """Layout-aware extraction from OCR items."""
    h, _w = img_shape_hw
    y_tol = max(12.0, 0.02 * h)
    highlights: Dict[str, Tuple[int, int, int, int]] = {}

    full_text = " ".join(it.text_norm for it in items)
    full_text = re.sub(r"\s+", " ", full_text).strip()

    # Tc: 11 digits
    tc = ""
    for it in items:
        digits = re.sub(r"\D", "", it.text_norm)
        if len(digits) == 11:
            tc = digits
            highlights["Tc"] = (it.x1, it.y1, it.x2, it.y2)
            break
    if not tc:
        tc = safe_regex_find(r"\b\d{11}\b", full_text) or ""

    # Date of birth
    dob_pat = r"\b\d{2}[./-]\d{2}[./-]\d{4}\b"
    dob = ""
    for it in items:
        m = safe_regex_find(dob_pat, it.text_norm)
        if m:
            dob = m.replace("-", ".").replace("/", ".")
            highlights["DateofBirth"] = (it.x1, it.y1, it.x2, it.y2)
            break
    if not dob:
        m = safe_regex_find(dob_pat, full_text)
        dob = (m.replace("-", ".").replace("/", ".") if m else "")

    surname = ""
    name = ""

    lbl_surname = find_label(items, ["SOYADI", "SOYAD"])
    if lbl_surname:
        words = pick_line_words(items, lbl_surname.cy, y_tol, lbl_surname.x2 + 5)
        surname = join_words(words)
        if words:
            x1 = min(w.x1 for w in words)
            y1 = min(w.y1 for w in words)
            x2 = max(w.x2 for w in words)
            y2 = max(w.y2 for w in words)
            highlights["Surname"] = (x1, y1, x2, y2)

    lbl_name = find_label(items, ["ADI", "AD"])
    if lbl_name:
        words = pick_line_words(items, lbl_name.cy, y_tol, lbl_name.x2 + 5)
        name = join_words(words)
        if words:
            x1 = min(w.x1 for w in words)
            y1 = min(w.y1 for w in words)
            x2 = max(w.x2 for w in words)
            y2 = max(w.y2 for w in words)
            highlights["Name"] = (x1, y1, x2, y2)

    out = {"Tc": tc, "Surname": surname, "Name": name, "DateofBirth": dob}
    return out, highlights


def annotate(img_bgr: np.ndarray, items: List[OcrItem], highlights: Dict[str, Tuple[int, int, int, int]], draw_all: bool) -> np.ndarray:
    out = img_bgr.copy()
    if draw_all:
        for it in items:
            cv2.rectangle(out, (it.x1, it.y1), (it.x2, it.y2), (0, 255, 0), 1)

    for field, (x1, y1, x2, y2) in highlights.items():
        cv2.rectangle(out, (x1, y1), (x2, y2), (0, 0, 255), 2)
        cv2.putText(out, field, (x1, max(0, y1 - 6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
    return out


# ----------------------------
# ----------------------------
# Repo "PIPELINE" mode (ported from main.py)
# ----------------------------
ORI_THRESH = 3  # Orientation angle threshold for skew correction


def getCenterRatios(img: np.ndarray, centers: np.ndarray) -> np.ndarray:
    """Return (cx/img_w, cy/img_h) for each center."""
    if img.ndim == 2:
        img_h, img_w = img.shape
    else:
        img_h, img_w = img.shape[:2]

    ratios = np.zeros_like(centers, dtype=np.float32)
    for i, (cx, cy) in enumerate(centers):
        ratios[i] = (float(cx) / float(img_w), float(cy) / float(img_h))
    return ratios


def matchCenters(ratios1: np.ndarray, ratios2: np.ndarray) -> Tuple[int, int, int, int]:
    """Map 4 mask-centers (ratios1) to the closest CRAFT box-centers (ratios2)."""
    if ratios1.shape[0] != 4:
        raise ValueError(f"Attesi 4 centri dalla maschera, trovati {ratios1.shape[0]}")
    if ratios2.shape[0] == 0:
        raise ValueError("Nessun box rilevato da CRAFT.")

    diffs = []
    for k in range(4):
        d = np.abs(ratios2 - ratios1[k])  # (N,2)
        diffs.append(np.sum(d, axis=1))   # (N,)

    idxs = [int(np.argmin(d)) for d in diffs]
    return idxs[0], idxs[1], idxs[2], idxs[3]


def getCenterOfMasks(mask: np.ndarray) -> np.ndarray:
    """Find centers of 4 largest mask components, sorted top-to-bottom."""
    m = mask.copy()
    if m.ndim != 2:
        raise ValueError("Maschera UNet non valida (attesa 2D).")

    if m.max() <= 1:
        m = (m * 255).astype(np.uint8)
    else:
        m = m.astype(np.uint8)

    contours, _ = cv2.findContours(m, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        raise ValueError("Nessun contorno trovato nella maschera UNet.")

    # keep 4 largest (by area), then sort by y (top-to-bottom)
    contours = sorted(contours, key=cv2.contourArea, reverse=True)[:4]
    bounding = [cv2.boundingRect(c) for c in contours]
    contours = [c for c, _bb in sorted(zip(contours, bounding), key=lambda p: p[1][1])]

    centers: List[Tuple[int, int]] = []
    for c in contours:
        x, y, w, h = cv2.boundingRect(c)
        centers.append((int(round(x + w / 2.0)), int(round(y + h / 2.0))))
    return np.array(centers, dtype=np.int32)


def getBoxRegions(regions: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """Convert CRAFT polygons to (x,w,y,h) boxes + centers."""
    boxes: List[Tuple[int, int, int, int]] = []
    centers: List[Tuple[int, int]] = []
    for box_region in regions:
        pts = np.array(box_region).reshape(-1).astype(int)
        if pts.size != 8:
            # Some CRAFT outputs can be shaped differently; enforce 4 points.
            pts = np.array(box_region).reshape(4, 2).astype(int).reshape(-1)
        x1, y1, x2, y2, x3, y3, x4, y4 = pts.tolist()

        x = min(x1, x3)
        y = min(y1, y2)
        w = abs(min(x1, x3) - max(x2, x4))
        h = abs(min(y1, y2) - max(y3, y4))

        cX = int(round(x + w / 2.0))
        cY = int(round(y + h / 2.0))
        centers.append((cX, cY))
        boxes.append((int(x), int(w), int(y), int(h)))

    return np.array(boxes, dtype=np.int32), np.array(centers, dtype=np.int32)


@st.cache_resource(show_spinner=False)
def _get_craft(cuda: bool):
    from craft_text_detector import Craft  # type: ignore
    return Craft(output_dir="outputs", crop_type="poly", cuda=cuda)


@st.cache_resource(show_spinner=False)
def _get_unet_res34(device: str):
    """Load UNet (ResNet34 backbone) once and reuse.

    Repo variants:
    - unet_predict.py at project root
    - pytorch_unet/unet_predict.py (as in the reference main.py)
    This loader supports both, plus a final fallback to a direct file-path import.
    """
    Res34BackBone = None  # type: ignore

    # 1) Preferred: package path used by the reference repo
    try:
        from pytorch_unet.unet_predict import Res34BackBone as _Res34BackBone  # type: ignore
        Res34BackBone = _Res34BackBone
    except Exception:
        pass

    # 2) Fallback: root-level module
    if Res34BackBone is None:
        try:
            from unet_predict import Res34BackBone as _Res34BackBone  # type: ignore
            Res34BackBone = _Res34BackBone
        except Exception:
            pass

    # 3) Final fallback: load by file path (works even if pytorch_unet isn't a package)
    if Res34BackBone is None:
        import importlib.util

        base = Path(__file__).resolve().parent
        candidates = [
            base / "unet_predict.py",
            base / "pytorch_unet" / "unet_predict.py",
        ]

        last_err = None
        for cand in candidates:
            if cand.exists():
                try:
                    spec = importlib.util.spec_from_file_location("_unet_predict_dyn", cand)
                    if spec and spec.loader:
                        mod = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(mod)  # type: ignore[attr-defined]
                        Res34BackBone = getattr(mod, "Res34BackBone")
                        break
                except Exception as e:
                    last_err = e

        if Res34BackBone is None:
            raise ModuleNotFoundError(
                "Impossibile importare Res34BackBone da 'unet_predict' o 'pytorch_unet/unet_predict.py'. "
                "Assicurati che il file esista nel progetto."
            ) from last_err

    backbone = Res34BackBone()  # type: ignore[operator]
    model = backbone.load_model(device)
    return backbone, model

@st.cache_resource(show_spinner=False)
def _get_face_detector(method: str):
    if not _FACE_FACTORY_AVAILABLE:
        return None
    return face_factory(method).get_face_detector()


def _create_heatmap_and_regions(img_bgr: np.ndarray, cuda: bool) -> Tuple[np.ndarray, np.ndarray]:
    """CRAFT text score heatmap + box polygons."""
    craft = _get_craft(cuda=cuda)
    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    pred = craft.detect_text(img_rgb)
    heatmap = pred["heatmaps"]["text_score_heatmap"]
    regions = pred["boxes"]
    return heatmap, regions


def _to_3ch_float(img: np.ndarray) -> np.ndarray:
    """Ensure HxWx3 float32."""
    if img.ndim == 2:
        out = np.stack([img, img, img], axis=-1)
    elif img.ndim == 3 and img.shape[2] == 1:
        out = np.repeat(img, 3, axis=2)
    elif img.ndim == 3 and img.shape[2] == 3:
        out = img
    else:
        raise ValueError(f"Formato heatmap non supportato: shape={img.shape}")
    return out.astype(np.float32)


def pipeline_extract(img_bgr: np.ndarray, img_name: str, ocr_method: str, neighbor_dist: int) -> Tuple[Dict[str, str], np.ndarray]:
    """Pipeline completa (come main.py): CRAFT -> UNet -> matchCenters -> NearestBox -> OCR."""
    if not _UTILS_AVAILABLE:
        raise RuntimeError("utlis.py non importabile: impossibile usare la modalit√† PIPELINE.")

    # Lazily import heavy deps only in PIPELINE
    try:
        import torch
        device = "cuda" if torch.cuda.is_available() else "cpu"
        cuda_for_craft = torch.cuda.is_available()
    except Exception:
        device = "cpu"
        cuda_for_craft = False

    try:
        from find_nearest_box import NearestBox  # type: ignore
    except Exception as e:
        raise RuntimeError(f"Impossibile importare find_nearest_box.py: {e}")

    try:
        from extract_words import OcrFactory  # type: ignore
    except Exception as e:
        raise RuntimeError(f"Impossibile importare extract_words.py: {e}")

    # UNet weights + segmentation_models_pytorch must be available
    try:
        backbone, unet_model = _get_unet_res34(device=device)
    except Exception as e:
        raise RuntimeError(
            "Impossibile caricare il modello UNet (ResNet34). " 
            "Controlla che 'segmentation-models-pytorch' sia installato e che i file in 'model/resnet34/' esistano. " 
            f"Dettaglio: {e}"
        )

    # 1) CRAFT heatmap + text boxes
    heatmap, regions = _create_heatmap_and_regions(img_bgr, cuda=cuda_for_craft)
    heatmap_3ch = _to_3ch_float(heatmap)

    # 2) UNet predicted mask
    predicted_mask = backbone.predict(unet_model, heatmap_3ch, device)
    predicted_mask_u8 = predicted_mask.astype(np.uint8)

    # 3) Orientation correction using mask lines (main.py logic)
    orientation_angle = utlis.findOrientationofLines(predicted_mask_u8.copy())
    if orientation_angle is not None and abs(float(orientation_angle)) > ORI_THRESH:
        img_bgr = utlis.rotateImage(float(orientation_angle), img_bgr)
        heatmap, regions = _create_heatmap_and_regions(img_bgr, cuda=cuda_for_craft)
        heatmap_3ch = _to_3ch_float(heatmap)
        predicted_mask = backbone.predict(unet_model, heatmap_3ch, device)
        predicted_mask_u8 = predicted_mask.astype(np.uint8)

    # 4) Convert boxes + find centers
    if regions is None or len(regions) == 0:
        raise RuntimeError("CRAFT non ha rilevato box di testo.")
    bbox_coordinates, box_centers = getBoxRegions(np.array(regions))
    mask_centers = getCenterOfMasks(predicted_mask_u8)

    # 5) Match mask centers -> nearest craft centers (ratio-based)
    centers_ratio_mask = getCenterRatios(predicted_mask_u8, mask_centers)
    centers_ratio_all = getCenterRatios(img_bgr, box_centers)
    matched_box_indexes = matchCenters(centers_ratio_mask, centers_ratio_all)

    # 6) Expand to nearest neighbor boxes (main.py)
    nearestBox = NearestBox(distance_thresh=float(neighbor_dist), draw_line=False)
    new_bboxes = nearestBox.searchNearestBoundingBoxes(bbox_coordinates, matched_box_indexes, img_bgr)

    # 7) OCR on the 4 final regions
    Image2Text = OcrFactory.select_ocr_method(ocr_method=ocr_method, border_thresh=3, denoise=False)
    person_info = Image2Text.ocrOutput(img_name, img_bgr, new_bboxes)

    # 8) Annotate output
    out = img_bgr.copy()
    labels = ["Tc", "Surname", "Name", "DateofBirth"]
    for lab, (x, w, y, h) in zip(labels, new_bboxes):
        x, w, y, h = int(x), int(w), int(y), int(h)
        cv2.rectangle(out, (x, y), (x + w, y + h), (0, 0, 255), 2)
        cv2.putText(out, lab, (x, max(0, y - 6)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

    return person_info, out


# Face alignment / perspective (optional)
# ----------------------------
def align_by_face(img_bgr: np.ndarray, method: str, rot_interval: int) -> Tuple[np.ndarray, str]:
    if not _FACE_FACTORY_AVAILABLE:
        return img_bgr, "Rilevamento volto non disponibile (detect_face.py non importabile)."

    try:
        detector = _get_face_detector(method)
        if detector is None:
            return img_bgr, "Rilevamento volto non disponibile (detect_face.py non importabile)."
        aligned = detector.changeOrientationUntilFaceFound(img_bgr, rot_interval)
        if aligned is None:
            return img_bgr, "Nessun volto rilevato: uso l'immagine originale."
        return aligned, "Allineamento tramite volto completato."
    except Exception as e:
        return img_bgr, f"Allineamento volto non riuscito: {e}. Uso l'immagine originale."


def apply_perspective(img_bgr: np.ndarray) -> Tuple[np.ndarray, str]:
    if not _UTILS_AVAILABLE:
        return img_bgr, "Correzione prospettiva non disponibile (utlis.py non importabile)."
    try:
        return utlis.correctPerspective(img_bgr), "Correzione prospettiva completata."
    except Exception as e:
        return img_bgr, f"Correzione prospettiva non riuscita: {e}. Uso l'immagine originale."


# ----------------------------
# Sidebar
# ----------------------------
st.sidebar.header("Impostazioni")

mode = st.sidebar.radio("Modalit√†", ["SIMPLE (consigliata)", "PIPELINE (sperimentale)"], index=0)

with st.sidebar.expander("Opzioni avanzate", expanded=True):
    face_method = st.selectbox("Metodo volto (rotazione)", ["ssd", "haar", "dlib"], index=0)
    rot_interval = st.slider("Passo rotazione (gradi)", 5, 30, 15)
    do_perspective = st.checkbox("Correzione prospettiva ID (se disponibile)", value=False)
    ocr_method = st.selectbox("Metodo OCR", ["TesseractOcr", "EasyOcr"], index=0)
    neighbor_dist = st.slider("Distanza NearestBox (solo PIPELINE)", 10, 100, 60)
    show_all_boxes = st.checkbox("Mostra tutti i box OCR (solo SIMPLE)", value=True)


uploaded_file = st.file_uploader(
    "Trascina qui la tua immagine o clicca per cercare",
    type=["jpg", "png", "jpeg", "heic", "bmp", "webp"],
    accept_multiple_files=False,
)


# ----------------------------
# Main
# ----------------------------
if uploaded_file is not None:
    pil_image = load_image(uploaded_file)
    if pil_image is None:
        st.stop()

    img_bgr = pil_to_bgr(pil_image)

    col1, col2 = st.columns(2)
    with col1:
        st.image(pil_image, caption="Immagine caricata", use_container_width=True)

    if st.button("Avvia OCR", type="primary"):
        with st.spinner("Elaborazione in corso..."):
            # 0) Face alignment
            img_bgr, msg_face = align_by_face(img_bgr, face_method, rot_interval)
            st.info(msg_face)

            # 1) Perspective correction (optional)
            if do_perspective:
                img_bgr, msg_p = apply_perspective(img_bgr)
                st.info(msg_p)

            # 2) Run selected mode
            try:
                if mode.startswith("PIPELINE"):
                    data, vis_bgr = pipeline_extract(
                        img_bgr=img_bgr,
                        img_name="streamlit_scan",
                        ocr_method=ocr_method,
                        neighbor_dist=int(neighbor_dist),
                    )
                else:
                    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

                    # SIMPLE: pick OCR backend
                    if ocr_method == "EasyOcr":
                        try:
                            items = run_easyocr(img_rgb)
                        except Exception as e:
                            st.warning(f"EasyOCR non disponibile ({e}). Passo a Tesseract.")
                            items = run_tesseract(img_rgb)
                    else:
                        items = run_tesseract(img_rgb)

                    data, highlights = extract_fields_simple(items, img_rgb.shape[:2])
                    vis_bgr = annotate(img_bgr, items, highlights, draw_all=show_all_boxes)

                # 3) Show result
                with col2:
                    st.image(cv2.cvtColor(vis_bgr, cv2.COLOR_BGR2RGB), caption="Risultato / Debug", use_container_width=True)

                st.divider()
                st.subheader("Dati estratti")
                st.json(data, expanded=True)

                json_str = json.dumps(data, indent=2, ensure_ascii=False)
                st.download_button(
                    label="Scarica JSON",
                    data=json_str,
                    file_name="risultato_ocr.json",
                    mime="application/json",
                )

            except Exception as e:
                st.error(f"Errore durante l'elaborazione: {e}")
